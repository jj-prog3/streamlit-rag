import streamlit as st
from pathlib import Path
import copy

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.documents import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.schema import AIMessage, HumanMessage

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="ğŸ“„ Chat with Document",
    page_icon="ğŸ“„",
    layout="wide"
)
st.title("ğŸ“„ ë¬¸ì„œ ê¸°ë°˜ ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜")

# --- ì‚¬ì´ë“œë°” ì„¤ì • ---
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")

    # 1. ì‚¬ìš©ì OpenAI API í‚¤ ì…ë ¥
    api_key = st.text_input(
        "OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”",
        type="password",
        help="API í‚¤ëŠ” OpenAI ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

    # 2. ë¬¸ì„œ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader(
        "ë¶„ì„í•  ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (.txt)",
        type=["txt"]
    )

    # 3. GitHub ë¦¬í¬ì§€í† ë¦¬ ë§í¬
    st.markdown("---")
    st.markdown(
        "â¤ï¸ [GitHub ë¦¬í¬ì§€í† ë¦¬](https://github.com/your-username/your-repo)"
    )


# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœì— í•„ìš”í•œ í‚¤ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    if "messages" not in st.session_state:
        st.session_state.messages = [
            AIMessage(content="ì•ˆë…•í•˜ì„¸ìš”! ë¶„ì„í•  ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì‹œì‘í•˜ì„¸ìš”.")
        ]
    if "retriever" not in st.session_state:
        st.session_state.retriever = None
    if "chain" not in st.session_state:
        st.session_state.chain = None
    if "processed_file_name" not in st.session_state:
        st.session_state.processed_file_name = None

initialize_session_state()


# --- í•µì‹¬ ë¡œì§: íŒŒì¼ ì²˜ë¦¬ ë° ì²´ì¸ ìƒì„± ---
def process_file_and_create_chain(api_key, uploaded_file):
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  RAG ì²´ì¸ì„ ìƒì„±í•˜ì—¬ ì„¸ì…˜ ìƒíƒœì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        # LLM ë° ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        llm = ChatOpenAI(temperature=0.1, max_tokens=1024, openai_api_key=api_key)
        embeddings = OpenAIEmbeddings(openai_api_key=api_key)

        # ì—…ë¡œë“œëœ íŒŒì¼ ì½ê¸°
        file_content = uploaded_file.getvalue().decode("utf-8")
        
        # ë¬¸ì„œë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
        raw_doc = [Document(page_content=file_content)]

        # í…ìŠ¤íŠ¸ ë¶„í• 
        splitter = CharacterTextSplitter.from_tiktoken_encoder(
            separator="\n",
            chunk_size=300,
            chunk_overlap=50,
        )
        docs = splitter.split_documents(raw_doc)

        # ìºì‹œ ì„¤ì • ë° ì„ë² ë”©
        cache_dir = LocalFileStore("./.cache/")
        cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
            embeddings, cache_dir
        )
        
        # ë²¡í„° ì €ì¥ì†Œ ìƒì„± (FAISS)
        vectorstore = FAISS.from_documents(docs, cached_embeddings)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 2}) # 2ê°œì˜ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
                    "ë¬¸ì„œ ë‚´ìš©ì— ì—†ëŠ” ì •ë³´ëŠ” ë‹µë³€í•˜ì§€ ë§ê³ , ëª¨ë¥¸ë‹¤ê³  ì†”ì§í•˜ê²Œ ë§í•˜ì„¸ìš”.\n\n"
                    "Context:\n{context}"
                ),
                MessagesPlaceholder(variable_name="history"),
                ("human", "{question}"),
            ]
        )

        # RAG ì²´ì¸ êµ¬ì„±
        chain = (
            {
                "context": retriever | (lambda docs: "\n\n".join(d.page_content for d in docs)),
                "question": RunnablePassthrough(),
                "history": lambda x: st.session_state.get('history', [])
            }
            | prompt
            | llm
            | StrOutputParser()
        )
        
        # ìƒì„±ëœ retrieverì™€ chainì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state.retriever = retriever
        st.session_state.chain = chain
        st.session_state.processed_file_name = uploaded_file.name
        
        return True
    
    except Exception as e:
        st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

# --- ë©”ì¸ í™”ë©´ ë¡œì§ ---

# API í‚¤ì™€ íŒŒì¼ì´ ëª¨ë‘ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸
if not api_key:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
elif not uploaded_file:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
else:
    # íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , ë³€ê²½ë˜ì—ˆë‹¤ë©´ ìƒˆë¡œ ì²˜ë¦¬
    if st.session_state.processed_file_name != uploaded_file.name:
        with st.spinner("â³ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            if process_file_and_create_chain(api_key, uploaded_file):
                st.success(f"'{uploaded_file.name}' ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                # ìƒˆ ë¬¸ì„œê°€ ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
                st.session_state.messages = [
                    AIMessage(content=f"'{uploaded_file.name}'ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.")
                ]


    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for msg in st.session_state.messages:
        role = "user" if isinstance(msg, HumanMessage) else "assistant"
        st.chat_message(role).write(msg.content)

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if user_query := st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì²´ì¸ì´ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if st.session_state.chain:
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
            st.session_state.messages.append(HumanMessage(content=user_query))
            st.chat_message("user").write(user_query)

            with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                # RAG ì²´ì¸ ì‹¤í–‰
                response = st.session_state.chain.invoke(
                    user_query,
                    # LangChain Expression Language (LCEL)ì„ ì‚¬ìš©í•˜ë©´, 
                    # historyëŠ” prompt í…œí”Œë¦¿ ë‚´ì—ì„œ lambda í•¨ìˆ˜ë¥¼ í†µí•´ ë™ì ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
                )

                # AI ì‘ë‹µ ì¶”ê°€ ë° í‘œì‹œ
                st.session_state.messages.append(AIMessage(content=response))
                st.chat_message("assistant").write(response)
                
                # ë‹¤ìŒ í˜¸ì¶œì„ ìœ„í•´ history ì—…ë°ì´íŠ¸
                st.session_state['history'] = st.session_state.messages[-2:] # ìµœê·¼ ì§ˆë¬¸ê³¼ ë‹µë³€ë§Œ íˆìŠ¤í† ë¦¬ë¡œ ê´€ë¦¬

        else:
            st.warning("ë¬¸ì„œê°€ ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")